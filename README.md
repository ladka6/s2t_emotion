---
license: mit
task_categories:
  - audio-classification
  - text-generation
language:
  - en
tags:
  - emotion-recognition
  - speech
  - empathy
  - conversational-ai
size_categories:
  - 1K<n<10K
---

# Emotion-Aware Speech Dataset with Embeddings

This dataset contains processed audio samples from RAVDESS and CREMA-D datasets, enriched with:

- **Pre-computed embeddings** (emotion + speaker)
- **Emotion labels** (automatically detected)
- **Transcripts** of spoken utterances
- **GPT-generated empathetic responses**

## Dataset Overview

- **Total samples**: 8882
  - Train: 7105
  - Validation: 888
  - Test: 889
- **Embedding dimension**: 960 (concatenated emotion + speaker embeddings)
- **Source datasets**: RAVDESS, CREMA-D

## Dataset Structure

Each sample contains:

```python
{
    'audio_path': str,              # Path to original audio file
    'transcript': str,              # Transcribed text
    'emotion_label': str,           # Detected emotion (e.g., 'angry', 'happy', 'sad')
    'combined_embedding': List[float],  # Concatenated emotion + speaker embeddings
    'assistant_reply': str,         # GPT-generated empathetic response
    'context': List[str],           # Conversation context (currently empty)
    'dataset_source': str,          # 'RAVDESS' or 'CREMA-D'
}
```

## Features

### Embeddings

- **Emotion embeddings**: Extracted using Wav2Vec2 model trained on IEMOCAP
- **Speaker embeddings**: Extracted using ECAPA-TDNN model trained on VoxCeleb
- **Combined**: Concatenated into a single feature vector

### Emotion Labels

Automatically detected emotions include:

- Neutral
- Happy
- Sad
- Angry
- Fear
- Disgust
- Surprise

### Assistant Replies

Empathetic responses generated by GPT-3.5-turbo based on:

- The spoken transcript
- The detected emotion

## Usage

```python
from datasets import load_dataset

# Load the dataset
dataset = load_dataset("ladka6/emotion-speech-embeddings")

# Access splits
train_data = dataset['train']
val_data = dataset['validation']
test_data = dataset['test']

# Example: Access first sample
sample = train_data[0]
print(f"Transcript: {sample['transcript']}")
print(f"Emotion: {sample['emotion_label']}")
print(f"Assistant reply: {sample['assistant_reply']}")
print(f"Embedding shape: {len(sample['combined_embedding'])}")
```

```

```
